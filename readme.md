基于BERT的蒸馏实验
================

参考论文《Distilling Task-Specific Knowledge from BERT into Simple Neural Networks》

分别采用keras和pytorch基于textcnn和bilstm(gru)进行了实验

实验数据分割成 1（有标签训练）：8（无标签训练）：1（测试）

在情感2分类clothing的数据集上初步结果如下：

 - 小模型（textcnn & bilstm）准确率在 0.80 ~ 0.81

 - BERT模型 准确率在 0.90 ~ 0.91

 - 蒸馏模型 准确率在 0.87 ~ 0.88

实验结果与论文结论基本一致，与预期相符

后续将尝试其他更有效的蒸馏方案

## 使用方法

首先finetune BERT
```bash
python ptbert.py
```

然后把BERT的知识蒸馏到小模型里

需要先解压`data/cache/word2vec.gz`

然后
```bash
python distill.py
```

调整文件中的`use_aug`及以下的参数可以使用论文中提到的其中两种数据增强方式(masking, n-gram sampling)

# 实验数据

```bash
python distill.py
```

MLP-mixer

```
0.18671476008727195 0.8535
0.1363012600980752 0.84775
0.10923628168220216 0.85125
0.10894649174619228 0.8815
0.11742356341622823 0.88725
0.08354068830186594 0.887625
0.08319684956883285 0.88725
0.056479572325099446 0.887375
0.05328394247694535 0.888875
0.05416255799313703 0.88775
```

RNN

```
0.18652671628387263 0.797875
0.14467526031724104 0.8435
0.11329939020863662 0.86825
0.08116068783486989 0.889625
0.0574096327924982 0.88325
0.04670551804997397 0.882625
0.034060617713984234 0.879875
0.028893109440724266 0.88
0.02427852056682744 0.879375
0.021115490895900743 0.8765
```

CNN

```
0.1843904212434241 0.847875
0.1242247907880773 0.871875
0.09974226060964114 0.886625
0.0927621832526956 0.88225
0.08209383630372108 0.88375
0.0833655381905483 0.8845
0.0677793304178309 0.890125
0.06656713504344225 0.885
0.08213516197279624 0.885375
0.0687697584631833 0.8865
```
